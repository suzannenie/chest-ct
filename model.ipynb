{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone https://github.com/jfhealthcare/Chexpert and move CheXpert dataset inside this directory\n",
    "Inference for 5 classes with pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suzannenie/Library/Python/myvenv3/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from easydict import EasyDict as edict\n",
    "import json\n",
    "from torch.nn import DataParallel\n",
    "from model.classifier import Classifier\n",
    "import time\n",
    "\n",
    "# model = torch.load(\"config/pre_train.pth\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg_path = \"config/example.json\"\n",
    "pre_train = \"config/pre_train.pth\"\n",
    "\n",
    "with open(cfg_path) as f:\n",
    "    cfg = edict(json.load(f))\n",
    "    # print(json.dumps(cfg, indent=4))\n",
    "\n",
    "# device_ids = list(map(int, args.device_ids.split(',')))\n",
    "device_ids = []\n",
    "num_devices = torch.cuda.device_count()\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda:{}'.format(device_ids[0]))\n",
    "\n",
    "\n",
    "model = Classifier(cfg)\n",
    "model = DataParallel(model, device_ids=device_ids).to(device).train()\n",
    "ckpt = torch.load(pre_train, map_location=device)\n",
    "model.module.load_state_dict(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from data.dataset import ImageDataset\n",
    "import numpy as np\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    ImageDataset(cfg[\"dev_csv\"], cfg, mode='heatmap'),\n",
    "    batch_size=1, num_workers=4,\n",
    "    drop_last=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "model.eval()\n",
    "device = torch.device('cpu')\n",
    "steps = len(dataloader)\n",
    "dataiter = iter(dataloader)\n",
    "num_tasks = len(cfg.num_classes)\n",
    "txt_file = \"plot.txt\"\n",
    "\n",
    "test_header = [\n",
    "    'Path',\n",
    "    'Cardiomegaly',\n",
    "    'Edema',\n",
    "    'Consolidation',\n",
    "    'Atelectasis',\n",
    "    'Pleural Effusion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(output, cfg):\n",
    "    if cfg.criterion == 'BCE' or cfg.criterion == \"FL\":\n",
    "        for num_class in cfg.num_classes:\n",
    "            assert num_class == 1\n",
    "        pred = torch.sigmoid(output.view(-1)).cpu().detach().numpy()\n",
    "    elif cfg.criterion == 'CE':\n",
    "        for num_class in cfg.num_classes:\n",
    "            assert num_class >= 2\n",
    "        prob = F.softmax(output)\n",
    "        pred = prob[:, 1].cpu().detach().numpy()\n",
    "    else:\n",
    "        raise Exception('Unknown criterion : {}'.format(cfg.criterion))\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path,Cardiomegaly,Edema,Consolidation,Atelectasis,Pleural Effusion\n",
      "\n",
      "steps 234 num_tasks 5\n",
      "step  0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [1, 2048, 1, 1], expected input[1, 1024, 16, 16] to have 2048 channels, but got 1024 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m image, path, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataiter)\n\u001b[1;32m      9\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 10\u001b[0m output, __ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(path)\n\u001b[1;32m     12\u001b[0m pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((num_tasks, batch_size))\n",
      "File \u001b[0;32m~/Library/Python/myvenv3/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/myvenv3/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:150\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(\u001b[39m\"\u001b[39m\u001b[39mDataParallel.forward\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    149\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids:\n\u001b[0;32m--> 150\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    152\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m chain(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39mparameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39mbuffers()):\n\u001b[1;32m    153\u001b[0m         \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mdevice \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msrc_device_obj:\n",
      "File \u001b[0;32m~/Library/Python/myvenv3/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Programming_projects/Chest_CT/Chexpert/model/classifier.py:147\u001b[0m, in \u001b[0;36mClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    144\u001b[0m logit_map \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ( \u001b[39m#self.cfg.global_pool == 'AVG_MAX' or\u001b[39;00m\n\u001b[1;32m    146\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mglobal_pool \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mAVG_MAX_LSE\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 147\u001b[0m     logit_map \u001b[39m=\u001b[39m classifier(feat_map)\n\u001b[1;32m    148\u001b[0m     logit_maps\u001b[39m.\u001b[39mappend(logit_map\u001b[39m.\u001b[39msqueeze())\n\u001b[1;32m    149\u001b[0m \u001b[39m# (N, C, 1, 1)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/myvenv3/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/myvenv3/lib/python3.8/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/Library/Python/myvenv3/lib/python3.8/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [1, 2048, 1, 1], expected input[1, 1024, 16, 16] to have 2048 channels, but got 1024 channels instead"
     ]
    }
   ],
   "source": [
    "print(','.join(test_header) + '\\n')\n",
    "print(\"steps\", steps, \"num_tasks\", num_tasks)\n",
    "steps = 5\n",
    "images = []\n",
    "\n",
    "for step in range(steps):\n",
    "    print(\"step \", step)\n",
    "    image, path, labels = next(dataiter)\n",
    "    image = image.to(device)\n",
    "    output, __ = model(image)\n",
    "    batch_size = len(path)\n",
    "    pred = np.zeros((num_tasks, batch_size))\n",
    "\n",
    "    for i in range(num_tasks):\n",
    "        pred[i] = get_pred(output[i], cfg)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        batch = ','.join(map(lambda x: '{}'.format(x), pred[:, i]))\n",
    "        result = path[i] + ',' + batch\n",
    "        print(result + '\\n')\n",
    "        print('{}, Image : {}, Prob : {}, labels : {} '.format(\n",
    "            time.strftime(\"%Y-%m-%d %H:%M:%S\"), path[i], batch, labels))\n",
    "        print('\\n\\n')\n",
    "        images.append(path[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CheXpert-v1.0-small/valid/patient64541/study1/view1_frontal.jpg\n",
      "torch.Size([1, 3, 512, 512]) (320, 390, 3)\n",
      "[tensor([[0.4413]]), tensor([[0.0234]]), tensor([[-1.1419]]), tensor([[-0.1762]]), tensor([[-1.9679]])] []\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects a non-empty TensorList",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m jpg_file \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(jpg_file)\n\u001b[0;32m---> 29\u001b[0m prefix, figure_data \u001b[38;5;241m=\u001b[39m \u001b[43mheatmaper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen_heatmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjpg_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m bn \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(jpg_file)\n\u001b[1;32m     31\u001b[0m save_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(plot_path, prefix, bn)\n",
      "File \u001b[0;32m~/Documents/Programming_projects/Chest_CT/Chexpert/util/heatmaper.py:131\u001b[0m, in \u001b[0;36mHeatmaper.gen_heatmap\u001b[0;34m(self, image_file)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mprint\u001b[39m(logits, logit_maps)\n\u001b[1;32m    130\u001b[0m logits \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(logits)\n\u001b[0;32m--> 131\u001b[0m logit_maps \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mstack(logit_maps)\n\u001b[1;32m    132\u001b[0m \u001b[39m# tensor to numpy\u001b[39;00m\n\u001b[1;32m    133\u001b[0m image_np \u001b[39m=\u001b[39m tensor2numpy(image_tensor)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
     ]
    }
   ],
   "source": [
    "from util.heatmaper import Heatmaper \n",
    "# Heatmaper = None\n",
    "\n",
    "disease_classes = [\n",
    "    'Cardiomegaly',\n",
    "    'Edema',\n",
    "    'Consolidation',\n",
    "    'Atelectasis',\n",
    "    'Pleural Effusion'\n",
    "]\n",
    "plot_path = \"plots\"\n",
    "alpha = 0.2\n",
    "prefix = \"none\"\n",
    "\n",
    "cfg[\"global_pool\"] = \"AVG_MAX\"\n",
    "\n",
    "# create plot folder\n",
    "if not os.path.exists(plot_path):\n",
    "    os.mkdir(plot_path)\n",
    "# construct heatmap_cfg\n",
    "heatmaper = Heatmaper(alpha, prefix, cfg, model, device)\n",
    "assert prefix in ['none', *(disease_classes)]\n",
    "with open(txt_file) as f:\n",
    "    # for line in f:\n",
    "    for line in images:\n",
    "        time_start = time.time()\n",
    "        jpg_file = line.strip('\\n')\n",
    "        print(jpg_file)\n",
    "        prefix, figure_data = heatmaper.gen_heatmap(jpg_file)\n",
    "        bn = os.path.basename(jpg_file)\n",
    "        save_file = '{}/{}{}'.format(plot_path, prefix, bn)\n",
    "        assert cv2.imwrite(save_file, figure_data), \"write failed!\"\n",
    "        time_spent = time.time() - time_start\n",
    "        print(\n",
    "            '{}, {}, heatmap generated, Run Time : {:.2f} sec'\n",
    "            .format(time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                    jpg_file, time_spent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from data.utils import transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512, 512])\n",
      "(320, 390, 3)\n"
     ]
    }
   ],
   "source": [
    "image_file = images[0]\n",
    "\n",
    "image_gray = cv2.imread(image_file, 0)\n",
    "assert image_gray is not None, \"invalid image read in: {}\"\\\n",
    "    .format(image_file)\n",
    "image_color = cv2.cvtColor(image_gray, cv2.COLOR_GRAY2RGB)\n",
    "image = transform(image_gray, cfg)\n",
    "image = torch.from_numpy(image)\n",
    "image = image.unsqueeze(0)\n",
    "print(image.shape)\n",
    "print(image_color.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([[-2.9403]]),\n",
       "  tensor([[-0.3623]]),\n",
       "  tensor([[-1.0317]]),\n",
       "  tensor([[-0.6134]]),\n",
       "  tensor([[-2.3091]])],\n",
       " [])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv3",
   "language": "python",
   "name": "myvenv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
